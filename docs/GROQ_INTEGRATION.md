# Integraci√≥n de Groq con el Asistente de Spotify

## ¬øPor qu√© Groq?

Groq ofrece varias ventajas significativas sobre otros proveedores de modelos de IA:

1. **Velocidad superior**: Los modelos de Groq son extremadamente r√°pidos, ofreciendo respuestas con latencias mucho menores que OpenAI o Anthropic.
2. **L√≠mites generosos**: Groq ofrece cuotas de uso m√°s altas en su plan gratuito.
3. **Precios competitivos**: Menor costo por token que muchos competidores.
4. **Modelos de calidad**: Acceso a Llama 3 y Mixtral, que tienen excelente rendimiento.
5. **API compatible con OpenAI**: Facilita la migraci√≥n desde sistemas basados en OpenAI.

## Instalaci√≥n

Para a√±adir soporte para Groq a tu asistente, instala la dependencia oficial:

```bash
npm install groq-sdk
```

## Configuraci√≥n

1. Obt√©n una API key de [Groq](https://console.groq.com/).
2. A√±ade la API key a tu archivo `.env`:

```
GROQ_API_KEY=tu_api_key_aqu√≠
```

## Uso

Hemos creado un nuevo proveedor de modelos `modelProvider-groq.js` que soporta tanto Groq como OpenRouter con fallback autom√°tico. Para usarlo:

1. Renombra el archivo existente por seguridad:
```bash
mv backend/src/services/ai/modelProvider.js backend/src/services/ai/modelProvider.js.backup
```

2. Utiliza el nuevo proveedor:
```bash
cp backend/src/services/ai/modelProvider-groq.js backend/src/services/ai/modelProvider.js
```

3. Reinicia el servicio:
```bash
docker compose restart backend
```

## C√≥mo funciona

El nuevo proveedor de modelos implementa una estrategia de fallback:

1. Primero intenta usar **Groq** por su velocidad superior.
2. Si Groq falla o no est√° disponible, utiliza **OpenRouter**.
3. En cada proveedor, intenta m√∫ltiples modelos en orden de preferencia.

```
Usuario -> Solicitud -> modelProvider -> Groq -> OpenRouter -> Respuesta
```

## Modelos soportados

### Groq
- `llama3-8b-8192`: M√°s r√°pido, ideal para tareas simples
- `llama3-70b-8192`: M√°s poderoso, mejor para tareas complejas
- `mixtral-8x7b-32768`: Excelente para contextos largos

### OpenRouter (fallback)
- `openai/gpt-4o`
- `anthropic/claude-3-opus-20240229`
- `meta-llama/llama-3-70b-instruct`
- `google/gemini-pro`

## Personalizando los modelos

Puedes ajustar los modelos disponibles y su orden de prioridad modificando los arrays `MODELS.GROQ` y `MODELS.OPENROUTER` en `modelProvider.js`.

## Ventajas para el Asistente de Spotify

1. **Respuestas m√°s r√°pidas**: La baja latencia de Groq mejora significativamente la experiencia del usuario.
2. **Mayor fiabilidad**: El sistema de fallback autom√°tico garantiza que el asistente siempre responda.
3. **Optimizado para recomendaciones**: Se ajustan los par√°metros de temperatura y tokens seg√∫n el tipo de solicitud.
4. **Estructura JSON consistente**: Ambos proveedores utilizan el mismo formato, manteniendo la compatibilidad.

## Monitoreo y Depuraci√≥n

Los logs incluyen informaci√≥n detallada sobre qu√© proveedor y modelo se est√° utilizando, facilitando la depuraci√≥n:

```
‚úÖ Cliente Groq inicializado correctamente
ü§ñ Intentando con modelo Groq: llama3-8b-8192
‚úÖ Respuesta exitosa de Groq (modelo: llama3-8b-8192)
```

## Soluci√≥n de problemas

### Si no hay respuesta de ning√∫n proveedor

Verifica las API keys en tu archivo `.env`:
```
GROQ_API_KEY=tu_api_key_groq
OPENROUTER_API_KEY=tu_api_key_openrouter
```

### Si las respuestas no tienen el formato correcto

Revisa la funci√≥n `validateResponse` que garantiza que todas las respuestas cumplan con el esquema requerido por el asistente.
